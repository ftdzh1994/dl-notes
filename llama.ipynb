{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7842, 0.1649, 0.0308],\n",
      "        [0.5024, 0.9393, 0.4886]])\n",
      "tensor([[1.6937, 0.3562, 0.0665],\n",
      "        [0.7425, 1.3882, 0.7222]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    \"\"\"docstring for RMSNorm.\"\"\"\n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "    \n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        x = self._norm(x)\n",
    "        return self.weight * x.to(input_dtype)\n",
    "    \n",
    "    # 实现代码2\n",
    "    # def forward1(self, x):\n",
    "    #     output = self._norm(x.float()).type_as(x)\n",
    "    #     return self.weight * output\n",
    "    \n",
    "# RMSNorm test\n",
    "x = torch.rand(2, 3)\n",
    "print(x)\n",
    "r = RMSNorm(x.size(-1))\n",
    "print(r(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False) #persistent=False将不会作为state_dict\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n",
    "        )\n",
    "        \n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        #超过预设的max_position_embeddings则重新计算更大的Rope缓存，否则直接在缓存上切片\n",
    "        if seq_len > self.max_seq_len_cached: \n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
    "            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    \n",
    "    #此处逻辑与原始的ROPE有所差异，原始逻辑如下\n",
    "    #x1 = x[..., 0::2] \n",
    "    #x2 = x[..., 1::2]\n",
    "    #res = torch.cat((x1, x2), dim=-1)\n",
    "    #res[...,0::2]=-x2\n",
    "    #res[...,1::2]=x1\n",
    "    #return res\n",
    "    \n",
    "    x1 = x[..., : x.shape[-1] // 2] \n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
    "    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n",
    "    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 8])\n",
      "tensor([[[[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000],\n",
      "          [ 0.5403,  0.9950,  0.9999,  1.0000,  0.5403,  0.9950,  0.9999,\n",
      "            1.0000],\n",
      "          [-0.4161,  0.9801,  0.9998,  1.0000, -0.4161,  0.9801,  0.9998,\n",
      "            1.0000],\n",
      "          [-0.9900,  0.9553,  0.9996,  1.0000, -0.9900,  0.9553,  0.9996,\n",
      "            1.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,8,4,2)\n",
    "rope = LlamaRotaryEmbedding(dim=8)\n",
    "cos,sin = rope.forward(x,seq_len=4)\n",
    "print(cos.shape) \n",
    "print(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250,\n",
       "        0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656,\n",
       "        0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062,\n",
       "        0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469,\n",
       "        0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875,\n",
       "        0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281,\n",
       "        0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688,\n",
       "        0.9844])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 128\n",
    "torch.arange(0, dim, 2)[:(dim //2)].float() / dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.1232e-17+1.0000j, -1.4142e+00-1.4142j], dtype=torch.complex128)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "abs = torch.tensor([1, 2], dtype=torch.float64)\n",
    "angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64)\n",
    "z = torch.polar(abs, angle)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(32, 128)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = x[..., : x.shape[-1] // 2] \n",
    "print(x1.size())\n",
    "x2 = x[..., x.shape[-1] // 2 :]\n",
    "torch.cat((-x2, x1), dim=-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[[-0.9850, -2.2869],\n",
       "            [ 1.4804, -0.7685],\n",
       "            [ 0.5947, -0.8731],\n",
       "            [-0.9780,  0.7070]],\n",
       "\n",
       "           [[ 0.4824,  0.6247],\n",
       "            [ 0.3330, -0.8440],\n",
       "            [-2.0476,  1.5151],\n",
       "            [ 0.2602,  0.1515]],\n",
       "\n",
       "           [[ 0.3006, -0.2799],\n",
       "            [ 1.6850,  1.4726],\n",
       "            [-0.2695,  0.4490],\n",
       "            [ 0.8820, -2.0848]],\n",
       "\n",
       "           [[-0.0554, -1.0951],\n",
       "            [-0.1584,  0.7699],\n",
       "            [ 0.5442, -0.0554],\n",
       "            [ 0.7802, -0.5489]],\n",
       "\n",
       "           [[-1.0475,  1.6059],\n",
       "            [-1.1929, -0.5152],\n",
       "            [-0.0339,  1.4537],\n",
       "            [ 1.2854,  0.8239]],\n",
       "\n",
       "           [[-0.4184, -1.2918],\n",
       "            [ 0.7324, -0.5918],\n",
       "            [-0.7403,  0.6899],\n",
       "            [ 0.7254,  0.4882]],\n",
       "\n",
       "           [[-0.4869,  0.2572],\n",
       "            [-0.2051,  0.6930],\n",
       "            [-0.9310,  0.6159],\n",
       "            [ 0.1253, -0.3576]],\n",
       "\n",
       "           [[ 1.4549,  0.6704],\n",
       "            [-1.4409,  0.6654],\n",
       "            [-0.2931, -0.6642],\n",
       "            [-0.6512, -0.2051]]]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,8,4,2)\n",
    "x[None, None, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
