{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, p_dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_attention_heads == 0\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        \n",
    "        self.w_q = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.w_k = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.w_v = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=p_dropout)\n",
    "        self.out_proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        '''\n",
    "        Args:\n",
    "            query:(n, b, h * d)\n",
    "            key: (m, b, h * d)\n",
    "            value: (m, b, h * d)\n",
    "            mask: (n, m)\n",
    "        '''\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "            \n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "            for lin, x in zip((self.w_q, self.w_k, self.w_v), (query, key, value))\n",
    "        ]\n",
    "        \n",
    "        attention_scores = query @ key.transpose(-1, -2)\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        # print(attention_scores.size())\n",
    "        if mask is not None:\n",
    "        #* masked_fill_(mask, value) Fills elements of self tensor with value where mask is True\n",
    "        #* 使用-1e9而不是0用于平滑概率\n",
    "            attention_scores = attention_scores.masked_fill(mask == False, -1e9)\n",
    "        \n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        # print(attention_probs)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        # mask\n",
    "        \n",
    "        print(attention_probs.size())\n",
    "        print(value.size())\n",
    "        attn_output = attention_probs @ value\n",
    "        print(attn_output.size())\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(nbatches, -1, self.attention_head_size * self.num_attention_heads)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        del query, key, value\n",
    "        \n",
    "        return attn_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 5, 5])\n",
      "torch.Size([3, 2, 5, 7])\n",
      "torch.Size([3, 2, 5, 7])\n",
      "torch.Size([3, 2, 5, 5])\n",
      "torch.Size([3, 2, 5, 7])\n",
      "torch.Size([3, 2, 5, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1199,  0.1243, -0.1686, -0.1893, -0.1900,  0.0054,  0.1330,\n",
       "          -0.0005, -0.1163, -0.1168,  0.1957, -0.2002, -0.1375, -0.0690],\n",
       "         [ 0.1470,  0.1595, -0.1004, -0.1963, -0.1259, -0.0901,  0.2416,\n",
       "          -0.0398, -0.0937,  0.0846,  0.2394, -0.2158, -0.0494, -0.1611],\n",
       "         [ 0.0830,  0.1671, -0.2460, -0.1067, -0.2407,  0.0781,  0.1123,\n",
       "           0.0213, -0.1483, -0.1111,  0.1597, -0.1293, -0.2468, -0.1078],\n",
       "         [ 0.1192,  0.1229, -0.1105, -0.1635, -0.2022, -0.0157,  0.1115,\n",
       "           0.0268, -0.0923, -0.1301,  0.1943, -0.2377, -0.1230, -0.0905],\n",
       "         [ 0.1682,  0.1337, -0.2225, -0.1858, -0.2076,  0.1221,  0.0628,\n",
       "          -0.0404, -0.0194, -0.1219,  0.2253, -0.2234, -0.2180, -0.0507]],\n",
       "\n",
       "        [[ 0.3645,  0.3415, -0.0063,  0.4457, -0.2646,  0.0780,  0.0881,\n",
       "          -0.0082,  0.2552, -0.0368,  0.3430, -0.3450,  0.1320, -0.1877],\n",
       "         [ 0.4069,  0.2339, -0.0800,  0.0998, -0.3201,  0.0527,  0.1317,\n",
       "          -0.0243,  0.1409,  0.1563,  0.4060, -0.2921, -0.0529, -0.2721],\n",
       "         [ 0.2541,  0.3084, -0.2087,  0.5154, -0.2782,  0.0338, -0.0308,\n",
       "           0.1602,  0.2455, -0.2135,  0.3013, -0.3138, -0.0164, -0.2346],\n",
       "         [ 0.4420,  0.2634, -0.0574,  0.1685, -0.2103,  0.0669,  0.1930,\n",
       "          -0.0646,  0.1640,  0.2218,  0.3671, -0.3423, -0.0684, -0.2452],\n",
       "         [ 0.3462,  0.1717, -0.1170,  0.0837, -0.2290,  0.1012,  0.1910,\n",
       "          -0.1385,  0.2576, -0.0005,  0.3633, -0.3869, -0.0471, -0.1020]],\n",
       "\n",
       "        [[-0.1137,  0.1088, -0.0927,  0.3219, -0.0822, -0.0660,  0.1475,\n",
       "           0.0119,  0.0675,  0.1236,  0.0157, -0.4540, -0.1130, -0.2340],\n",
       "         [-0.0027,  0.1359, -0.0879,  0.1256, -0.1319, -0.0672,  0.0797,\n",
       "           0.0986,  0.0212,  0.1527,  0.0667, -0.3781, -0.1005, -0.3042],\n",
       "         [ 0.0791,  0.2507, -0.1648,  0.2589, -0.1134, -0.1677,  0.1351,\n",
       "          -0.0145,  0.0818,  0.3397,  0.1238, -0.4555, -0.0436, -0.3535],\n",
       "         [-0.1020,  0.0422,  0.0105,  0.2633, -0.0452,  0.0326,  0.2602,\n",
       "          -0.0402,  0.0775,  0.1791, -0.0477, -0.4854, -0.1763, -0.1760],\n",
       "         [-0.0717,  0.1046,  0.0015,  0.2422, -0.0514, -0.0218,  0.2364,\n",
       "           0.0267,  0.0016,  0.2319, -0.0677, -0.4534, -0.1834, -0.2371]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbatch, seq_length, hidden_size = 3, 5, 14\n",
    "mask = torch.triu(torch.ones(3, 5, 5), diagonal=1) == 0\n",
    "# print(mask)\n",
    "mha = MultiHeadAttention(hidden_size, 2)\n",
    "\n",
    "q = k = v = torch.randn(nbatch, seq_length, hidden_size)\n",
    "# print(mha(q, k, v, mask))\n",
    "# print(mha(q, k, v))\n",
    "mha(q, k, v, mask)\n",
    "mha(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输入数据，假设seq_length为10\n",
    "src = torch.rand((10, 32, 512))  # (seq_length, batch_size, d_model)\n",
    "\n",
    "# 创建一个padding mask，其中填充位置为True\n",
    "padding_mask = (src == 0).all(dim=-1)  # 假设填充的值为0\n",
    "padding_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个模块\n",
    "transformer_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "\n",
    "# 输入数据，假设seq_length为10\n",
    "src = torch.rand((32, 10, 512))  # (seq_length, batch_size, d_model)\n",
    "\n",
    "# 创建一个padding mask，其中填充位置为True\n",
    "padding_mask = (src == 0).all(dim=-1)  # 假设填充的值为0\n",
    "\n",
    "# 将padding mask的形状扩展到匹配src的形状\n",
    "# padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)  # (seq_length, 1, 1, batch_size)\n",
    "\n",
    "# 调用forward函数时传入padding_mask参数\n",
    "output = transformer_layer(src, src_key_padding_mask=padding_mask.transpose(0, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-5, elementwise_affine=True):\n",
    "        super(MyLayerNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        if elementwise_affine:\n",
    "            self.a_2 = nn.Parameter(torch.ones(features))\n",
    "            self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        else:\n",
    "            self.a_2 = torch.ones(features)\n",
    "            self.b_2 = torch.zeros(features)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self) -> None:\n",
    "        if self.elementwise_affine:\n",
    "            nn.init.ones_(self.a_2)\n",
    "            nn.init.zeros_(self.b_2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1,keepdim=True)\n",
    "        return  self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.6084,  1.2828,  0.3061, -0.8851,  0.8505,  0.1779, -0.2187,\n",
      "          -0.3460, -1.0200,  0.4999],\n",
      "         [-0.3927,  1.5157,  0.2072,  2.3811,  0.1161, -0.0482,  0.5108,\n",
      "          -0.6616, -1.0974,  1.1873]],\n",
      "\n",
      "        [[-0.6979,  0.9624,  0.4424,  1.6685, -1.3118, -1.9541,  0.2597,\n",
      "           1.1270, -2.2716, -0.7226],\n",
      "         [ 0.1440,  0.1903, -1.6607,  0.2088, -0.9958,  1.2468, -0.2510,\n",
      "           0.5317,  0.1493, -0.8226]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.8130,  1.6980,  0.4012, -1.1804,  1.1241,  0.2310, -0.2955,\n",
      "          -0.4645, -1.3594,  0.6585],\n",
      "         [-0.7213,  1.0793, -0.1553,  1.8957, -0.2413, -0.3963,  0.1311,\n",
      "          -0.9750, -1.3863,  0.7694]],\n",
      "\n",
      "        [[-0.3327,  0.9000,  0.5139,  1.4242, -0.7884, -1.2653,  0.3783,\n",
      "           1.0222, -1.5010, -0.3510],\n",
      "         [ 0.3228,  0.3783, -1.8357,  0.4003, -1.0404,  1.6419, -0.1496,\n",
      "           0.7866,  0.3292, -0.8333]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "N, S, H = 2, 2, 10\n",
    "input = torch.randn(N, S, H)\n",
    "\n",
    "layer_norm_op = nn.LayerNorm([N, S, H], elementwise_affine=True) \n",
    "ln_y = layer_norm_op(input)\n",
    "\n",
    "my_layer_norm = MyLayerNorm(input.size(), elementwise_affine=True)\n",
    "verify_ln_y = my_layer_norm(input)\n",
    "\n",
    "print(ln_y)\n",
    "print(verify_ln_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('M', 1000), ('CM', 900), ('D', 500), ('CD', 400), ('C', 100), ('XC', 90), ('L', 50), ('XL', 40), ('X', 10), ('IX', 9), ('V', 5), ('IV', 4), ('I', 1)]\n"
     ]
    }
   ],
   "source": [
    "roman_map = [(1000, 'M'), \n",
    "             (900, 'CM'), (500, 'D'), (400, 'CD'), (100, 'C'),\n",
    "             (90, 'XC'), (50, 'L'), (40, 'XL'), (10, 'X'),\n",
    "             (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')]\n",
    "\n",
    "roman_map = [(v, k) for k, v in roman_map] \n",
    "print(roman_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
