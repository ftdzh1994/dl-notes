{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backward propagation\n",
    "\n",
    "##### 思路\n",
    "1. 建立一个类似于pytorch的神经网络类\n",
    "2. 验证反向传播值是否正确\n",
    "3. 如何训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement via Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n",
      "torch.Size([32, 5])\n",
      "tensor(0.9904, grad_fn=<MseLossBackward0>)\n",
      "Parameter: layers.0.weight, Gradient: tensor([[ 1.3943e-04,  4.6572e-04, -3.3065e-04,  3.5841e-04,  1.5319e-04,\n",
      "         -1.4903e-04, -6.0108e-04, -3.7794e-04,  1.4852e-04,  1.0288e-04],\n",
      "        [-2.8646e-04,  2.4914e-04,  1.0915e-03,  4.0133e-07, -9.6309e-05,\n",
      "         -1.1860e-03, -3.2893e-04, -5.4088e-04, -9.7305e-04,  4.5672e-04],\n",
      "        [-3.1324e-04,  1.3100e-04, -5.3196e-04,  2.7500e-04, -6.6554e-05,\n",
      "          2.8644e-04, -7.0296e-05, -2.5064e-04,  6.9971e-04, -3.4421e-04],\n",
      "        [ 4.1571e-04, -4.3944e-05,  9.2465e-04, -3.3053e-04,  2.4716e-04,\n",
      "         -6.8029e-04, -3.0840e-05,  3.1085e-04, -1.1051e-03,  6.4344e-04],\n",
      "        [ 4.4178e-05,  7.7999e-06,  2.0557e-04,  2.9671e-04, -2.0258e-04,\n",
      "         -4.4942e-04, -1.3752e-04, -5.1561e-04, -2.9037e-04, -1.9371e-04],\n",
      "        [-2.2254e-04,  4.2136e-05, -2.7577e-04, -1.3525e-04, -1.5094e-05,\n",
      "          2.4938e-04,  6.0952e-05,  1.1661e-04,  3.0963e-04,  7.9430e-05],\n",
      "        [ 5.5567e-04,  1.1520e-04,  3.3953e-04, -3.2557e-04,  3.8960e-04,\n",
      "         -2.0671e-04, -5.9936e-06,  5.4730e-04, -6.3369e-04,  5.9204e-04],\n",
      "        [-5.0202e-04,  1.3030e-04,  1.1279e-04,  2.6601e-04, -4.5364e-04,\n",
      "         -7.0394e-04, -1.5778e-04, -7.0437e-04, -1.9915e-04, -5.2166e-05],\n",
      "        [-4.5869e-04,  4.8077e-04,  1.1428e-04,  5.5027e-04, -4.1210e-04,\n",
      "         -9.2543e-04, -6.1868e-04, -1.0904e-03, -2.5540e-04, -2.0810e-05],\n",
      "        [-5.8516e-04,  1.2846e-04, -5.4897e-04,  5.3300e-04,  6.0390e-05,\n",
      "          8.9574e-04, -4.7277e-04, -4.0494e-04,  1.5394e-03, -9.2311e-04],\n",
      "        [-2.4285e-04, -2.4603e-04, -2.2614e-04,  9.8665e-05, -3.7010e-04,\n",
      "          5.7631e-05,  2.7396e-04, -1.7533e-04,  1.0355e-04, -3.1896e-04],\n",
      "        [-4.5125e-04, -8.4205e-05, -2.5969e-04,  9.9040e-05, -3.0555e-04,\n",
      "          1.2474e-04,  1.9236e-04, -1.9603e-04,  3.5391e-04, -2.5558e-04],\n",
      "        [ 2.7369e-04,  1.8331e-05,  5.3325e-05, -9.0288e-05,  2.8879e-04,\n",
      "          2.4413e-04, -2.0123e-05,  3.0501e-04,  2.5601e-05,  7.2336e-05],\n",
      "        [ 4.8383e-04,  2.5795e-04,  6.2246e-04,  6.7968e-05,  2.3406e-04,\n",
      "         -2.9591e-04, -4.6838e-04, -1.5809e-04, -4.5494e-04,  2.2137e-05],\n",
      "        [ 8.4395e-04,  2.2289e-04,  1.7018e-05, -3.2716e-05,  2.0741e-04,\n",
      "         -1.2306e-04, -4.0609e-04,  1.7740e-05, -3.5695e-04,  2.6528e-04],\n",
      "        [-8.3062e-04, -3.1592e-04, -5.7672e-04,  4.2977e-04, -1.3871e-03,\n",
      "         -1.0058e-03,  2.7487e-04, -1.1777e-03, -1.8999e-04, -3.7947e-04],\n",
      "        [ 6.3284e-04,  4.5108e-04,  3.8616e-04,  1.5109e-05,  6.2618e-04,\n",
      "         -6.0917e-05, -6.1978e-04,  2.5495e-04, -2.4107e-04,  2.7547e-04],\n",
      "        [-2.1461e-04, -3.6473e-04, -3.3673e-04,  2.8830e-05, -6.9744e-04,\n",
      "         -4.6539e-04,  3.7159e-04, -3.0083e-04, -3.2659e-04,  6.9606e-05],\n",
      "        [-4.2081e-04,  2.4560e-04, -5.6962e-04,  1.0376e-04,  7.9874e-05,\n",
      "          1.7199e-04, -1.3154e-04, -1.3698e-04,  4.4305e-04,  1.6375e-04],\n",
      "        [-1.9088e-04, -8.0373e-04,  2.8982e-04, -2.3003e-04, -8.3042e-04,\n",
      "         -2.4575e-04,  7.7517e-04, -3.0652e-05, -4.4898e-04, -3.6120e-04]])\n",
      "Parameter: layers.0.bias, Gradient: tensor([-6.0049e-05,  8.2383e-04, -8.0473e-04,  1.4352e-03,  4.7064e-06,\n",
      "        -3.0692e-04,  1.2122e-03, -5.2236e-04, -3.5493e-04, -1.4630e-03,\n",
      "        -6.2252e-04, -7.7664e-04,  3.8453e-04,  9.9582e-04,  8.0557e-04,\n",
      "        -1.7694e-03,  1.1935e-03, -6.4577e-04, -5.8017e-04, -5.4370e-04])\n",
      "Parameter: layers.2.weight, Gradient: tensor([[ 0.0022,  0.0019,  0.0025,  0.0012,  0.0009,  0.0033,  0.0019,  0.0014,\n",
      "          0.0017,  0.0020,  0.0022,  0.0028,  0.0019,  0.0019,  0.0012,  0.0012,\n",
      "          0.0018,  0.0007,  0.0012,  0.0015],\n",
      "        [ 0.0022,  0.0020,  0.0020,  0.0032,  0.0028,  0.0026,  0.0029,  0.0020,\n",
      "          0.0020,  0.0019,  0.0017,  0.0013,  0.0023,  0.0022,  0.0020,  0.0027,\n",
      "          0.0025,  0.0031,  0.0023,  0.0021],\n",
      "        [ 0.0023,  0.0022,  0.0024,  0.0022,  0.0020,  0.0028,  0.0023,  0.0018,\n",
      "          0.0019,  0.0020,  0.0020,  0.0021,  0.0022,  0.0022,  0.0017,  0.0019,\n",
      "          0.0023,  0.0018,  0.0018,  0.0018],\n",
      "        [ 0.0009,  0.0014,  0.0014,  0.0016,  0.0016,  0.0012,  0.0016,  0.0012,\n",
      "          0.0008,  0.0009,  0.0013,  0.0011,  0.0014,  0.0012,  0.0008,  0.0008,\n",
      "          0.0022,  0.0012,  0.0014,  0.0011],\n",
      "        [ 0.0049,  0.0057,  0.0055,  0.0057,  0.0053,  0.0054,  0.0049,  0.0039,\n",
      "          0.0044,  0.0041,  0.0049,  0.0050,  0.0052,  0.0051,  0.0047,  0.0039,\n",
      "          0.0064,  0.0039,  0.0055,  0.0050],\n",
      "        [ 0.0038,  0.0041,  0.0042,  0.0034,  0.0030,  0.0042,  0.0029,  0.0025,\n",
      "          0.0035,  0.0032,  0.0036,  0.0041,  0.0035,  0.0038,  0.0036,  0.0026,\n",
      "          0.0038,  0.0020,  0.0036,  0.0036],\n",
      "        [ 0.0069,  0.0070,  0.0069,  0.0074,  0.0063,  0.0086,  0.0070,  0.0050,\n",
      "          0.0060,  0.0061,  0.0067,  0.0069,  0.0066,  0.0068,  0.0062,  0.0057,\n",
      "          0.0078,  0.0054,  0.0070,  0.0067],\n",
      "        [-0.0005, -0.0014, -0.0011, -0.0002, -0.0004,  0.0002,  0.0005, -0.0001,\n",
      "         -0.0007, -0.0002, -0.0008, -0.0012, -0.0006, -0.0007, -0.0011,  0.0005,\n",
      "         -0.0010,  0.0007, -0.0010, -0.0009],\n",
      "        [-0.0040, -0.0052, -0.0046, -0.0050, -0.0047, -0.0037, -0.0036, -0.0031,\n",
      "         -0.0039, -0.0032, -0.0040, -0.0041, -0.0043, -0.0044, -0.0046, -0.0031,\n",
      "         -0.0057, -0.0031, -0.0053, -0.0046],\n",
      "        [ 0.0074,  0.0072,  0.0074,  0.0080,  0.0069,  0.0090,  0.0076,  0.0055,\n",
      "          0.0064,  0.0064,  0.0067,  0.0068,  0.0071,  0.0072,  0.0063,  0.0065,\n",
      "          0.0080,  0.0063,  0.0070,  0.0067],\n",
      "        [-0.0031, -0.0019, -0.0025, -0.0026, -0.0019, -0.0046, -0.0034, -0.0022,\n",
      "         -0.0023, -0.0029, -0.0024, -0.0025, -0.0025, -0.0025, -0.0017, -0.0030,\n",
      "         -0.0020, -0.0027, -0.0017, -0.0020],\n",
      "        [ 0.0022,  0.0024,  0.0021,  0.0029,  0.0025,  0.0020,  0.0021,  0.0016,\n",
      "          0.0021,  0.0017,  0.0018,  0.0017,  0.0021,  0.0022,  0.0025,  0.0020,\n",
      "          0.0026,  0.0021,  0.0027,  0.0024],\n",
      "        [-0.0018, -0.0015, -0.0015, -0.0030, -0.0026, -0.0027, -0.0031, -0.0019,\n",
      "         -0.0015, -0.0018, -0.0016, -0.0010, -0.0020, -0.0018, -0.0013, -0.0026,\n",
      "         -0.0025, -0.0031, -0.0019, -0.0017],\n",
      "        [-0.0025, -0.0018, -0.0023, -0.0016, -0.0012, -0.0033, -0.0020, -0.0015,\n",
      "         -0.0020, -0.0022, -0.0019, -0.0024, -0.0019, -0.0021, -0.0016, -0.0019,\n",
      "         -0.0015, -0.0013, -0.0014, -0.0017],\n",
      "        [ 0.0019,  0.0015,  0.0019,  0.0015,  0.0013,  0.0023,  0.0017,  0.0013,\n",
      "          0.0015,  0.0016,  0.0014,  0.0016,  0.0016,  0.0017,  0.0012,  0.0016,\n",
      "          0.0014,  0.0013,  0.0012,  0.0013]])\n",
      "Parameter: layers.2.bias, Gradient: tensor([ 0.0036,  0.0049,  0.0044,  0.0025,  0.0103,  0.0071,  0.0138, -0.0009,\n",
      "        -0.0087,  0.0147, -0.0054,  0.0047, -0.0043, -0.0040,  0.0033])\n",
      "Parameter: layers.4.weight, Gradient: tensor([[ 3.7046e-02,  4.5470e-02,  3.0517e-02,  4.1899e-02,  3.8480e-02,\n",
      "          2.7205e-02,  3.4482e-02,  4.3363e-02,  2.9208e-02,  4.5130e-02,\n",
      "          4.2773e-02,  4.3967e-02,  4.4013e-02,  3.5337e-02,  3.1748e-02],\n",
      "        [-6.0004e-02, -7.1833e-02, -4.7032e-02, -6.5525e-02, -5.7727e-02,\n",
      "         -4.2105e-02, -5.0176e-02, -7.2006e-02, -4.5724e-02, -6.9704e-02,\n",
      "         -6.6250e-02, -6.5282e-02, -7.0307e-02, -5.4829e-02, -4.8377e-02],\n",
      "        [ 7.4467e-05,  4.0850e-04, -4.4519e-04,  4.7917e-05,  3.8236e-04,\n",
      "         -2.0148e-04,  1.1467e-03, -2.3802e-04, -7.1289e-04,  3.4710e-05,\n",
      "          4.7543e-04,  1.4581e-03, -6.0601e-04,  2.3722e-04,  1.2337e-03],\n",
      "        [-7.9292e-02, -9.6995e-02, -6.3135e-02, -8.7653e-02, -7.8809e-02,\n",
      "         -5.6619e-02, -7.0796e-02, -9.5669e-02, -5.9420e-02, -9.3638e-02,\n",
      "         -8.9449e-02, -9.0037e-02, -9.3254e-02, -7.2687e-02, -6.8107e-02],\n",
      "        [ 7.3867e-02,  8.8831e-02,  5.9664e-02,  8.2198e-02,  7.3722e-02,\n",
      "          5.3127e-02,  6.7025e-02,  8.5290e-02,  5.7386e-02,  8.7640e-02,\n",
      "          8.3859e-02,  8.5680e-02,  8.6114e-02,  6.9648e-02,  6.1577e-02]])\n",
      "Parameter: layers.4.bias, Gradient: tensor([ 7.5498e-02, -1.1935e-01,  1.5184e-04, -1.5969e-01,  1.4779e-01])\n"
     ]
    }
   ],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"docstring for SimpleNN.\"\"\"\n",
    "    def __init__(self, layers_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(layers_dim) - 1):\n",
    "            layers.append(nn.Linear(layers_dim[i], layers_dim[i+1]))\n",
    "            layers.append(nn.Sigmoid())\n",
    "        self.layers = nn.Sequential(*layers[:-1])        # Remove the last sigmoid layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# test & validate\n",
    "input_dim, output_dim = 10, 5\n",
    "batch_size = 32\n",
    "input_data = torch.randn(batch_size, input_dim)\n",
    "target_data = torch.randn(batch_size, output_dim)\n",
    "\n",
    "# initalized\n",
    "layer_dims = [input_dim, 20, 15, output_dim]\n",
    "model = SimpleNN(layer_dims)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# forward and backward propagation\n",
    "optimizer.zero_grad()\n",
    "output = model(input_data)\n",
    "print(input_data.shape)\n",
    "print(output.shape)\n",
    "loss = criterion(output, target_data)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "\n",
    "# 查看梯度信息\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"Parameter: {name}, Gradient: {param.grad}\")\n",
    "        \n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement 1 from scratch\n",
    "source from https://mp.weixin.qq.com/s/-3w3qZISIUb5OoshkCzOwQ\n",
    "\n",
    "code in line 33 is wrong, should be `z_temp =parameters[\"w\"+str(i)].dot(a[-1]) + parameters[\"b\"+str(i)]`\n",
    "\n",
    "尝试使用relu代替sigmoid，发现以下几点：\n",
    "1. relu要使用在足够深的神经网络中\n",
    "2. relu是稀疏激活的\n",
    "3. 在实践中，relu 激活通常与正则化技术（例如 dropout 或批量归一化）结合使用\n",
    "4. 使用大型数据集进行训练：GPT 和 Transformer 等模型通常在大型数据集上进行训练，这有助于弥补 relu 激活的潜在缺点。有了足够多的数据，该模型就能够学习稳健的表示并很好地泛化，即使负输入的 ReLU 问题即将解决。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 生成权重以及偏执项layers_dim代表每层的神经元个数，\n",
    "#比如[2,3,1]代表一个三成的网络，输入为2层，中间为3层输出为1层\n",
    "def init_parameters(layers_dim):\n",
    "    \n",
    "    L = len(layers_dim)\n",
    "    parameters ={}\n",
    "    for i in range(1,L):\n",
    "        parameters[\"w\"+str(i)] = np.random.random([layers_dim[i],layers_dim[i-1]])\n",
    "        parameters[\"b\"+str(i)] = np.zeros((layers_dim[i],1))\n",
    "    return parameters\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "# sigmoid的导函数\n",
    "def sigmoid_prime(z):\n",
    "        return sigmoid(z) * (1-sigmoid(z))\n",
    "\n",
    "# 前向传播，需要用到一个输入x以及所有的权重以及偏执项，都在parameters这个字典里面存储\n",
    "# 最后返回会返回一个caches里面包含的 是各层的a和z，a[layers]就是最终的输出\n",
    "def forward(x,parameters):\n",
    "    a = []\n",
    "    z = []\n",
    "    caches = {}\n",
    "    a.append(x)\n",
    "    z.append(x)\n",
    "    layers = len(parameters)//2\n",
    "    # 前面都要用sigmoid\n",
    "    for i in range(1,layers):\n",
    "        z_temp =parameters[\"w\"+str(i)].dot(a[-1]) + parameters[\"b\"+str(i)]\n",
    "        z.append(z_temp)\n",
    "        a.append(sigmoid(z_temp))\n",
    "    # 最后一层不用sigmoid\n",
    "    z_temp = parameters[\"w\"+str(layers)].dot(a[layers-1]) + parameters[\"b\"+str(layers)]\n",
    "    z.append(z_temp)\n",
    "    a.append(z_temp)\n",
    "    \n",
    "    caches[\"z\"] = z\n",
    "    caches[\"a\"] = a    \n",
    "    return  caches,a[layers]\n",
    "\n",
    "# 反向传播，parameters里面存储的是所有的各层的权重以及偏执，caches里面存储各层的a和z\n",
    "# al是经过反向传播后最后一层的输出，y代表真实值\n",
    "# 返回的grades代表着误差对所有的w以及b的导数\n",
    "def backward(parameters,caches,al,y):\n",
    "    layers = len(parameters)//2\n",
    "    grades = {}\n",
    "    m = y.shape[1]\n",
    "    # 假设最后一层不经历激活函数\n",
    "    # 就是按照上面的图片中的公式写的\n",
    "    grades[\"dz\"+str(layers)] = al - y\n",
    "    grades[\"dw\"+str(layers)] = grades[\"dz\"+str(layers)].dot(caches[\"a\"][layers-1].T) /m\n",
    "    grades[\"db\"+str(layers)] = np.sum(grades[\"dz\"+str(layers)],axis = 1,keepdims = True) /m\n",
    "    # 前面全部都是sigmoid激活\n",
    "    for i in reversed(range(1,layers)):\n",
    "        grades[\"dz\"+str(i)] = parameters[\"w\"+str(i+1)].T.dot(grades[\"dz\"+str(i+1)]) * sigmoid_prime(caches[\"z\"][i])\n",
    "        grades[\"dw\"+str(i)] = grades[\"dz\"+str(i)].dot(caches[\"a\"][i-1].T)/m\n",
    "        grades[\"db\"+str(i)] = np.sum(grades[\"dz\"+str(i)],axis = 1,keepdims = True) /m\n",
    "    return grades   \n",
    "\n",
    "# 就是把其所有的权重以及偏执都更新一下\n",
    "def update_grades(parameters,grades,learning_rate):\n",
    "    layers = len(parameters)//2\n",
    "    for i in range(1,layers+1):\n",
    "        parameters[\"w\"+str(i)] -= learning_rate * grades[\"dw\"+str(i)]\n",
    "        parameters[\"b\"+str(i)] -= learning_rate * grades[\"db\"+str(i)]\n",
    "    return parameters\n",
    "# 计算误差值\n",
    "def compute_loss(al,y):\n",
    "    return np.mean(np.square(al-y))\n",
    "\n",
    "# 加载数据\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    加载数据集\n",
    "    \"\"\"\n",
    "    x = np.arange(0.0,1.0,0.01)\n",
    "    y =20* np.sin(2*np.pi*x)\n",
    "    # 数据可视化\n",
    "    plt.scatter(x,y)\n",
    "    return x,y\n",
    "#进行测试\n",
    "x,y = load_data()\n",
    "x = x.reshape(1,100)\n",
    "y = y.reshape(1,100)\n",
    "plt.scatter(x,y)\n",
    "parameters = init_parameters([1,25,1])\n",
    "al = 0\n",
    "for i in range(4000):\n",
    "    caches,al = forward(x, parameters)\n",
    "    grades = backward(parameters, caches, al, y)\n",
    "    parameters = update_grades(parameters, grades, learning_rate= 0.3)\n",
    "    if i %100 ==0:\n",
    "        print(compute_loss(al, y))\n",
    "plt.scatter(x,al)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement 2 from scratch\n",
    "source from [Nielsen' book](https://mp.weixin.qq.com/s/-3w3qZISIUb5OoshkCzOwQ) and [this github repo](https://github.com/Owly-dabs/Matrix-based_backprop_over_mini-batch/blob/main/network_matx.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ~/miniconda3/envs/nndl/bin/python python3.5\n",
    "\n",
    "\"\"\"\n",
    "network.py\n",
    "~~~~~~~~~~\n",
    "\n",
    "A module to implement the stochastic gradient descent learning algorithm for a feedforward neural network. Gradients are calculated using backpropagation.\n",
    "Code is intended to be simple, easily readable, and easily modifiable. It is not optimized, and omits many desirable features.\n",
    "\"\"\"\n",
    "\n",
    "# import library\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Network class\n",
    "class Network():\n",
    "\n",
    "    def __init__(self,sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y,x) for x,y in zip(sizes[:-1],sizes[1:])]\n",
    "\n",
    "    def feedforward(self,a):\n",
    "        for b,w in zip(self.biases,self.weights):\n",
    "            a = sigmoid(np.dot(w,a) + b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self,training_data,epochs,mini_batch_size,eta,test_data=None):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch stochastic gradient descent.\n",
    "        The 'training_data' is a list of tuples (x,y) representing the \n",
    "        training inputs and the desired outputs.\n",
    "        The other non-optional parameters are self-explantory. \n",
    "\n",
    "        If 'test_data' is provided then the network will be evaluated against \n",
    "        the test data after each epoch, and partial progress printed out.\n",
    "        This is useful for tracking progress, but slows things down substantially.\n",
    "        \"\"\"\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        if test_data: \n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size] \n",
    "                for k in range(0,n,mini_batch_size)]\n",
    "\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch,eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {}: {}/{}\".format(j,self.evaluate(test_data),n_test))\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))\n",
    "            \n",
    "    def update_mini_batch(self,mini_batch,eta):\n",
    "        \"\"\"\n",
    "        Update the network's weights and biases by applying \n",
    "        gradient descent using back propagation to a single mini batch.\n",
    "        The 'mini_batch' is a list of tuples (x,y), and 'eta' is the learning rate\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        for x,y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x,y)\n",
    "            nabla_b = [nb+dnb for nb,dnb in zip(nabla_b,delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw,dnw in zip(nabla_w,delta_nabla_w)]\n",
    "        \n",
    "        self.weights = [w-(eta/len(mini_batch))*nw for w,nw in zip(self.weights,nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b,nb in zip(self.biases,nabla_b)]         \n",
    "\n",
    "    def backprop(self,x,y):\n",
    "        \"\"\"\n",
    "        Return a tuple `(nabla_b, nabla_w)` representing the gradient for the cost function C_x. \n",
    "        `nabla_b` and `nabla_w` are layer-by-layer lists of numpy arrays, similar to `self.biases` and `self.weights`.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1],y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "        # Note that the variable l in the loop below is used a little differently to the notation in Chapter 2 of the book. \n",
    "        # Here, l=1 means the last layer of neurons, l=2 is the second-last layer, and so on. \n",
    "        # It's a renumbering of the scheme in the book, used here to take advantage of the fact that python can use negative indices in lists.\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        \n",
    "        return(nabla_b,nabla_w)\n",
    "\n",
    "    def evaluate(self,test_data):\n",
    "        \"\"\"\n",
    "        Return the number of test inputs for which the neural network outputs the correct result.\n",
    "        Note that the neural network's output is assumed to be the index of whichever neuron in the final layer has the highest activation.\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)),y) for (x,y) in test_data]\n",
    "        return sum(int(x==y) for (x,y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations,y):\n",
    "        \"\"\"\n",
    "        Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations\n",
    "        \"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "\n",
    "#Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"\n",
    "    Derivative of the sigmoid function.\n",
    "    \"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement 2 from scratch -- matrix-based version\n",
    "https://github.com/Owly-dabs/Matrix-based_backprop_over_mini-batch/blob/main/network.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Network():\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y,x) for x,y in zip(sizes[:-1],sizes[1:])]\n",
    "    \n",
    "    def feedforward(self,a):\n",
    "        for b,w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w,a) + b)\n",
    "        return a\n",
    "\n",
    "    # SGD\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data):\n",
    "        # turn training_data from tuple into list\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "\n",
    "        for e in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                # x.shape = (number of examples in mini batch, (784, 1))\n",
    "                self.update_mini_batch(mini_batch,eta)\n",
    "\n",
    "            if test_data:\n",
    "                print(\"Epoch {}: {}/{}\".format(e,self.evaluate(test_data),n_test))\n",
    "            else:\n",
    "                print(\"Epoch {} complete.\".format(e))\n",
    "\n",
    "\n",
    "    # Update mini batch\n",
    "    def update_mini_batch(self,mini_batch,eta):\n",
    "        X = [x for x,y in mini_batch]\n",
    "        Y = [y for x,y in mini_batch]\n",
    "        nabla_b, nabla_w = self.backprop(X,Y)\n",
    "        \n",
    "        self.weights = [w-(eta/len(mini_batch))*nw \n",
    "                        for w,nw in zip(self.weights,nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                        for b,nb in zip(self.biases, nabla_b)]\n",
    "       \n",
    "\n",
    "    # backprop\n",
    "    def backprop(self, X, Y):\n",
    "        \"\"\"\n",
    "        Return a tuple `(nabla_b, nabla_w)` representing the gradient\n",
    "        for the cost_function C_x.\n",
    "        `nabla_b` and `nabla_w` are layer-by-layer lists of numpy arrays,\n",
    "        similar to `self.biases` and `self.weights`\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        # feedforward\n",
    "        activation = X # shape = (mini_batch_size, 784, 1) \n",
    "        activations = [X] # list to store all activations, layer-by-layer\n",
    "        Zs = [] # list to store all z vectors, layer-by-layer\n",
    "        for b,w in zip(self.biases,self.weights):\n",
    "            Z = np.transpose(np.dot(w, activation),(1,0,2)) + b\n",
    "            Zs.append(Z)\n",
    "            activation = sigmoid(Z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # backward pass\n",
    "        delta_batch = self.cost_derivative(activations[-1],Y) * \\\n",
    "            sigmoid_prime(Zs[-1]) # shape = (mini_batch_size,10,1)\n",
    "\n",
    "        ##TODO: figure out how to get delta to fit into nabla and \n",
    "        # match shapes with self.weights and self.biases\n",
    "        # Refer to shell \n",
    "        nabla_b[-1] = np.sum(delta_batch, axis=0)\n",
    "        nabla_w[-1] = np.squeeze(np.dot(np.transpose(delta_batch,(2,1,0)),np.transpose(activations[-2],(2,0,1))))\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            Z = Zs[-l]\n",
    "            sp = sigmoid_prime(Z)\n",
    "            delta_batch = np.transpose(np.dot(self.weights[-l+1].transpose(), delta_batch),(1,0,2)) * sp\n",
    "            nabla_b[-l] = np.sum(delta_batch,axis=0)\n",
    "            nabla_w[-l] = np.squeeze(np.dot(np.transpose(delta_batch,(2,1,0)), np.transpose(activations[-l-1],(2,0,1))))\n",
    "        \n",
    "        return(nabla_b,nabla_w)\n",
    "    def evaluate(self,test_data):\n",
    "        \"\"\"\n",
    "        Return the number of test inputs for which the neural network outputs the correct result.\n",
    "        Note that the neural network's output is assumed to be the index of whichever neuron in the final layer has the highest activation.\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)),y) for (x,y) in test_data]\n",
    "        return sum(int(x==y) for (x,y) in test_results)\n",
    "        \n",
    "    def cost_derivative(self,output_activations,y):\n",
    "        \"\"\"\n",
    "        Return the vector of partial derivatives (partial C_X/partial A)\n",
    "        for the output activations.\n",
    "        \"\"\"\n",
    "        return (output_activations-y)\n",
    "        \n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
